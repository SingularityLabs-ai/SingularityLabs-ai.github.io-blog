---
layout: post
title: Smallest Pretrained Models to download
date: 2023-11-12 09:18 +0530
---
| Model Name | Publisher | Layers | Parameters | Size (MB) |
|---|---|---|---|---|
| bert-base-uncased |Google AI| 12 | 109,482,268 | 110 |
| bert-large-uncased |Google AI| 24 | 340,103,232 | 400 |
| distilbert-base-uncased |Hugging Face| 6 | 66,343,232 | 36 |
| gpt2 |OpenAI | 12 | 117,509,920 | 450 |
| gpt3 |OpenAI | 24 | 175,477,920 | 760 |
| gpt-j |Google AI| 28 | 614,432,000 | 2,520 |
| roberta-base |Google AI| 12 | 137,265,792 | 166 |
| roberta-large |Google AI| 24 | 355,010,272 | 512 |
| albert-base-v2 |Google AI| 12 | 128,000,000 | 160 |
| albert-large-v2 |Google AI| 24 | 355,000,000 | 440 |
| electra-base Google AI|| 12 | 110,000,000 | 132 |
| electra-large |Google AI| 24 | 330,000,000 | 400 |
| distilbert-base-multilingual-cased |Hugging Face| 6 | 66,343,232 | 36 |
| distilbert-base-chinese |Hugging Face| 6 | 66,343,232 | 36 |


I hope this helps!
Please note that these are just a few examples, and there are many other open-source pretrained models available outside of Hugging Face. The size of these models can vary depending on the specific version and format of the model.

I hope this helps!

Please note that these are just a few examples, and there are many otherpretrained models available on Hugging Face. The size of these models can vary depending on the specific version and format of the model.
